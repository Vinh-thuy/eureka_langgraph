import os
from typing import TypedDict, Literal, List, Union
from langgraph.graph import StateGraph, END

from dotenv import load_dotenv
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser

# Importer les fonctions de cr√©ation des graphes des sous-agents
from .generic_chatbot_agent import create_generic_chatbot_graph # GenericChatbotAgentState n'est pas utilis√© directement ici
from .incident_analysis_agent import create_incident_analysis_graph # IncidentAnalysisAgentState n'est pas utilis√© directement ici

load_dotenv()

# Initialiser le client LangChain avec OpenAI
MODEL_NAME = "gpt-4o-mini"

llm = ChatOpenAI(
    model=MODEL_NAME,
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    temperature=0.1
)


# D√©finir le sch√©ma de sortie pour LangChain
class RouteDecision(BaseModel):
    step: Literal["generic_chatbot", "incident_analysis"] = Field(
        ...,  
        description="D√©termine si la demande doit √™tre rout√©e vers 'generic_chatbot' ou 'incident_analysis'."
    )

# Cr√©er le parseur de sortie
output_parser = JsonOutputParser(pydantic_object=RouteDecision)

# 1. D√©finir l'√©tat de l'orchestrateur
class OrchestratorState(TypedDict):
    question: str
    history: List[dict]  # Pour l'historique de chat
    routing_decision: Literal["generic_chatbot", "incident_analysis"]
    final_response: Union[str, dict]  # Pour stocker la r√©ponse finale
    conversation_context: dict  # Pour maintenir le contexte de conversation
    meta: dict  # Pour stocker les m√©tadonn√©es de la r√©ponse


# Sch√©ma pour la validation de la sortie structur√©e du LLM
class Route(BaseModel):
    step: Literal["generic_chatbot", "incident_analysis"] = Field(
        ...,  # Le champ est maintenant obligatoire
        description="D√©termine si la demande doit √™tre rout√©e vers 'generic_chatbot' ou 'incident_analysis'."
    )

# Configuration du routeur avec validation de la sortie
router = llm.with_structured_output(Route)


router_prompt = """
Tu es un routeur intelligent pour un syst√®me de support IT bancaire. 
Analyse la question et d√©termine si elle doit √™tre trait√©e par l'agent de chat g√©n√©rique ou par l'agent d'analyse d'incidents.

Voici les deux possibilit√©s √† consid√©rer :

1. Si la question concerne une demande d'analyse d'incident, 
   r√©ponds : "incident_analysis".
   Exemples : 
   - "analyse moi l'incident INC2309845"
   - "Explique moi l'incident INC2309846"
   
2. Pour toutes les autres questions g√©n√©rales, informations, demandes de renseignements ou conversations courantes,
   r√©ponds : "generic_chatbot".
   Exemples :
   - "Bonjour, comment allez-vous ?"
   - "Explique moi le fonctionnement de la voiture √©lectrique"

Ta r√©ponse doit √™tre UNIQUEMENT un des deux mots suivants : "generic_chatbot" ou "incident_analysis".
"""

# 2. D√©finir les n≈ìuds de l'orchestrateur
def route_question(state: OrchestratorState) -> dict:
    print("---ORCHESTRATEUR: ROUTAGE DE LA QUESTION---")
    
    # Initialiser les champs manquants
    if "question" not in state:
        state["question"] = ""
    
    if "conversation_context" not in state:
        state["conversation_context"] = {}
    
    context = state["conversation_context"]
    question = state["question"]
    
    # V√©rifier si on est d√©j√† dans une conversation d'incident
    in_incident_conv = context.get("in_incident_conversation", False)
    
    # Si on est d√©j√† dans une conversation d'incident, on reste sur l'agent d'incident
    if in_incident_conv:
        print("---ORCHESTRATEUR: RESTE DANS LA CONVERSATION D'INCIDENT---")
        print(f"[ROUTEUR] Contexte actuel: {context}")
        state["routing_decision"] = "incident_analysis"
        return state
    
    # Sinon, on route normalement
    try:
        print(f"[ROUTEUR] Question √† router: {question}")
        print(f"[ROUTEUR] Contexte actuel: {context}")
        
        # Appeler le routeur seulement si n√©cessaire
        if not question.strip():
            print("[ROUTEUR] Question vide, utilisation du chatbot g√©n√©rique par d√©faut")
            state["routing_decision"] = "generic_chatbot"
            return state
            
        # Appeler le routeur pour prendre une d√©cision
        decision = router.invoke([
            SystemMessage(content=router_prompt),
            HumanMessage(content=question),
        ])
        
        print(f"[ROUTEUR] D√©cision de routage: {decision.step}")
        
        # Mise √† jour de l'√©tat avec la d√©cision de routage
        state["routing_decision"] = decision.step
        
        # Si on commence une analyse d'incident, on marque le contexte
        if decision.step == "incident_analysis":
            print("[ROUTEUR] D√©tection d'une demande d'analyse d'incident")
            context["in_incident_conversation"] = True
            
            # Extraire l'ID d'incident si pr√©sent dans la question
            if "INC" in question.upper():
                import re
                match = re.search(r'INC\d+', question.upper())
                if match:
                    context["incident_id"] = match.group(0)
                    print(f"[ROUTEUR] ID d'incident extrait: {context['incident_id']}")
        
        return state
        
    except Exception as e:
        print(f"[ERREUR ROUTEUR] Erreur lors du routage: {str(e)}")
        # Fallback en cas d'√©chec
        state["routing_decision"] = "generic_chatbot"
        
        # En cas d'erreur, on nettoie le contexte d'incident pour √©viter les √©tats bloqu√©s
        if "in_incident_conversation" in context:
            print("[ERREUR ROUTEUR] Nettoyage du contexte d'incident suite √† une erreur")
            context.pop("in_incident_conversation", None)
            context.pop("incident_id", None)
    
    return state

def invoke_generic_chatbot_agent(state: OrchestratorState):
    print("---ORCHESTRATEUR: INVOCATION DE L'AGENT GENERIC CHATBOT---")
    
    # --- LOGIQUE DE BOUCLE DE CONVERSATION POUR LE CHATBOT G√âN√âRIQUE ---
    # Cette fonction assure la fluidit√© multi-tours en conservant l'historique et le contexte conversationnel.
    # Contrairement √† l'agent d'incident, il n'y a pas de collecte de donn√©es structur√©es, mais on maintient l'historique pour la coh√©rence des √©changes.
    
    # Initialiser les champs manquants dans l'√©tat
    if "question" not in state:
        state["question"] = ""
    if "conversation_context" not in state:
        state["conversation_context"] = {}
    if "history" not in state:
        state["history"] = []
    
    context = state["conversation_context"]
    question = state["question"]
    history = state["history"]
    
    print(f"[GENERIC CHATBOT] Contexte d'entr√©e: {context}")
    print(f"[GENERIC CHATBOT] Question: {question}")
    print(f"[GENERIC CHATBOT] Historique: {history[-3:]}")
    
    # --- Gestion de la transition depuis une conversation d'incident ---
    was_in_incident_conv = context.pop("in_incident_conversation", False)
    incident_id = context.pop("incident_id", None)
    if was_in_incident_conv:
        print("---GENERIC CHATBOT: FIN DE LA CONVERSATION D'INCIDENT---")
        if incident_id:
            print(f"[GENERIC CHATBOT] Incident pr√©c√©dent: {incident_id}")
    
    # --- D√©tection d'intention de sortie (NLU simple) ---
    exit_keywords = ["fin", "stop", "quitter", "merci", "termin√©", "au revoir"]
    if any(kw in question.lower() for kw in exit_keywords):
        print("[GENERIC CHATBOT] Fin de conversation d√©tect√©e (NLU)")
        # Nettoyage du contexte et de l'historique
        context.clear()
        history.clear()
        return {
            "final_response": "[üí¨ Chatbot g√©n√©rique]\nMerci pour cette conversation ! N'h√©sitez pas √† revenir si vous avez d'autres questions.",
            "conversation_context": context,
            "history": history,
            "meta": {
                "use_case": "generic_chatbot",
                "subgraph": "Generic Chatbot"
            },
            "question": question,
            "routing_decision": "generic_chatbot"
        }

    # --- Ajout de la question courante √† l'historique ---
    if question:
        history.append({"role": "user", "content": question})
    
    # --- G√©n√©ration de la r√©ponse via LLM (prompt simple, contexte = historique) ---
    try:
        # Appel direct au LLM avec l'historique (pas de prompt syst√®me sp√©cifique)
        messages = [HumanMessage(content=msg["content"]) if msg["role"] == "user" else SystemMessage(content=msg["content"]) for msg in history]
        llm_response = llm.invoke(messages)
        response = llm_response.content
        print(f"[GENERIC CHATBOT] R√©ponse g√©n√©r√©e: {response[:100]}...")
        # Ajout de la r√©ponse √† l'historique
        history.append({"role": "assistant", "content": response})
        # Retourner uniquement les champs d√©finis dans OrchestratorState
        return {
            "final_response": response,
            "conversation_context": context,
            "history": history,
            "meta": {
                "subgraph": "Chatbot g√©n√©rique",
                "use_case": "generic_chatbot"
            },
            "question": question,  # Conserver la question pour la coh√©rence
            "routing_decision": "generic_chatbot"
        }
    except Exception as e:
        print(f"[ERREUR GENERIC CHATBOT] Erreur lors de l'appel √† l'agent: {str(e)}")
        # En cas d'erreur, on nettoie le contexte pour √©viter les √©tats bloqu√©s
        context.pop("in_incident_conversation", None)
        context.pop("incident_id", None)
        return {
            "final_response": "D√©sol√©, une erreur est survenue lors du traitement de votre demande.",
            "conversation_context": context,
            "history": history,
            "meta": {
                "use_case": "generic_chatbot",
                "subgraph": "Generic Chatbot",
                "error": str(e)
            },
            "question": question,
            "routing_decision": "generic_chatbot"
        }

# ---
# Cette logique permet de conserver la fluidit√© et la m√©moire de la conversation pour le chatbot g√©n√©rique,
# m√™me sur plusieurs tours, sans workflow structur√© ni collecte de donn√©es, simplement en stockant l'historique.
# ---

def invoke_incident_analysis_agent(state: OrchestratorState):
    print("---ORCHESTRATEUR: INVOCATION DE L'AGENT INCIDENT ANALYSIS---")
    
    # Initialiser les champs manquants
    if "question" not in state:
        state["question"] = ""
    if "conversation_context" not in state:
        state["conversation_context"] = {}
    if "history" not in state:
        state["history"] = []
    
    context = state["conversation_context"]
    question = state["question"]
    
    print(f"[INCIDENT AGENT] Contexte d'entr√©e: {context}")
    print(f"[INCIDENT AGENT] Question: {question}")

    # D√©tection NLU (mots-cl√©s) de l'intention de sortie
    exit_keywords = ["fin", "stop", "quitter", "merci", "termin√©", "au revoir"]
    if any(kw in question.lower() for kw in exit_keywords):
        # Mot-cl√© de sortie d√©tect√©, on termine la conversation d'incident
        print("---INCIDENT AGENT: FIN DE LA CONVERSATION D'INCIDENT (mot-cl√© d√©tect√©)---")
        context["in_incident_conversation"] = False
        # Nettoyage de l'historique c√¥t√© backend
        if "history" in state:
            state["history"].clear()
        return {
            "final_response": "Conversation d‚Äôincident termin√©e. N‚Äôh√©sitez pas √† solliciter une nouvelle analyse.",
            "meta": {
                "use_case": "incident_analysis",
                "subgraph": "Analyse d'incident",
                "incident_id": context.get("incident_id")
            },
            "conversation_context": context,
            "history": []
        }
    
    # V√©rifier si on a un ID d'incident dans le contexte
    incident_id = context.get("incident_id")
    if not incident_id and "INC" in question.upper():
        # Essayer d'extraire l'ID d'incident de la question
        import re
        match = re.search(r'INC\d+', question.upper())
        if match:
            incident_id = match.group(0)
            context["incident_id"] = incident_id
            print(f"[INCIDENT AGENT] ID d'incident extrait: {incident_id}")
    # Toujours synchroniser l'ID d'incident courant pour le meta
    incident_id = context.get("incident_id")
    
    # V√©rifier si on est dans une conversation d'incident
    in_incident_conv = context.get("in_incident_conversation", False)
    
    # Si on n'est pas dans une conversation d'incident mais qu'on a un ID, on commence une nouvelle conversation
    if not in_incident_conv and incident_id:
        print(f"[INCIDENT AGENT] D√©but d'une nouvelle conversation pour l'incident {incident_id}")
        context["in_incident_conversation"] = True
        context["conversation_count"] = 1
    # Si on est d√©j√† dans une conversation, on incr√©mente le compteur
    elif in_incident_conv:
        context["conversation_count"] = context.get("conversation_count", 0) + 1
        print(f"[INCIDENT AGENT] Suite de la conversation (tour {context['conversation_count']})")
        
        # Appel normal √† l'agent d'analyse d'incident (plus de d√©tection NLU ici)
        try:
            # Pr√©parer l'entr√©e pour l'agent d'analyse d'incident
            incident_analysis_graph = create_incident_analysis_graph()
            incident_analysis_input = {
                "question": question,
                "conversation_context": context.copy(),
                "history": state["history"],
                "current_response": "",
                "end_conversation": False
            }
            print(f"[INCIDENT AGENT] Donn√©es envoy√©es √† l'agent: {incident_analysis_input}")
            final_agent_state = incident_analysis_graph.invoke(incident_analysis_input)
            if "conversation_context" in final_agent_state:
                print(f"[INCIDENT AGENT] Contexte retourn√©: {final_agent_state['conversation_context']}")
                context.update(final_agent_state["conversation_context"])
            else:
                print("[INCIDENT AGENT] Aucun contexte retourn√© par l'agent")
            # V√©rifier si la conversation est termin√©e
            end_conversation = final_agent_state.get("end_conversation", False)
            if end_conversation:
                print("---INCIDENT AGENT: FIN DE LA CONVERSATION D'INCIDENT---")
                context["in_incident_conversation"] = False
            # D√©terminer le nombre de tours de conversation
            conversation_count = context.get("conversation_count", 1)
            system_prompt = final_agent_state.get("system_prompt", "")
            # R√©cup√©rer la question utilisateur pour l'appel LLM
            question = state.get("question", "")
            # Par d√©faut, on renvoie la synth√®se courte (premier tour)
            response = final_agent_state.get(
                "final_response",
                final_agent_state.get(
                    "current_response",
                    final_agent_state.get(
                        "response",
                        "D√©sol√©, je n'ai pas pu traiter votre demande d'incident."
                    )
                )
            )
            print(f"[DEBUG] conversation_count={conversation_count}, system_prompt not empty? {bool(system_prompt)}")
            # Si on est dans la boucle conversationnelle (apr√®s synth√®se), on appelle le LLM avec le contexte enrichi
            if conversation_count > 1 and system_prompt:
                try:
                    from langchain_core.messages import SystemMessage, HumanMessage
                    llm_response = llm.invoke([
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=question)
                    ])
                    response = llm_response.content
                    print(f"[INCIDENT AGENT] R√©ponse LLM g√©n√©r√©e: {response[:100]}...")
                except Exception as e:
                    print(f"[INCIDENT AGENT] Erreur lors de l'appel LLM: {e}")
                    response = "D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse contextuelle."
            else:
                print(f"[INCIDENT AGENT] R√©ponse g√©n√©r√©e (synth√®se): {response[:100]}...")
            # Retourner la r√©ponse et le contexte mis √† jour
            return {
                "final_response": response,
                "meta": {
                    "use_case": "incident_analysis",
                    "subgraph": "Analyse d'incident",
                    "incident_id": context.get("incident_id")
                },
                "conversation_context": context
            }
        except Exception as e:
            print(f"[ERREUR INCIDENT AGENT] Erreur lors de l'appel √† l'agent: {str(e)}")
            # En cas d'erreur, on nettoie le contexte d'incident pour √©viter les √©tats bloqu√©s
            context.pop("in_incident_conversation", None)
            context.pop("incident_id", None)
            return {
                "final_response": "D√©sol√©, une erreur est survenue lors du traitement de votre demande d'incident.",
                "meta": {
                    "use_case": "incident_analysis",
                    "subgraph": "Analyse d'incident",
                    "incident_id": context.get("incident_id")
                },
                "conversation_context": context
            }

def fallback_node(state: OrchestratorState):
    print("---ORCHESTRATEUR: N≈íUD PAR D√âFAUT (FALLBACK)---")
    return {"final_response": "D√©sol√©, je ne peux pas traiter ce type de demande pour le moment. Essayez une recherche de recette ou d'appartement."}

# 3. D√©finir les ar√™tes conditionnelles
def decide_next_node(state: OrchestratorState):
    print("---ORCHESTRATEUR: D√âCISION DU PROCHAIN N≈íUD---")
    
    # Initialiser les champs manquants
    if "routing_decision" not in state:
        state["routing_decision"] = "generic_chatbot"
    if "conversation_context" not in state:
        state["conversation_context"] = {}
    
    context = state["conversation_context"]
    print(f"[DECIDE NEXT NODE] Contexte actuel: {context}")
    print(f"[DECIDE NEXT NODE] D√©cision de routage: {state['routing_decision']}")
    
    # Si on est dans une conversation d'incident, on reste sur l'agent d'incident
    if context.get("in_incident_conversation", False):
        print("[DECIDE NEXT NODE] Reste dans la conversation d'incident")
        return "incident_analysis_agent"  # Retourne le nom du n≈ìud tel que d√©fini dans create_orchestrator_graph
    
    # Si on a une d√©cision de routage, on la suit
    if state["routing_decision"] == "incident_analysis":
        print("[DECIDE NEXT NODE] Routage vers l'analyse d'incident")
        
        # Extraire l'ID d'incident de la question si possible
        question = state.get("question", "")
        if "INC" in question.upper():
            import re
            match = re.search(r'INC\d+', question.upper())
            if match:
                context["incident_id"] = match.group(0)
                print(f"[DECIDE NEXT NODE] ID d'incident extrait: {context['incident_id']}")
        
        return "incident_analysis_agent"  # Retourne le nom du n≈ìud tel que d√©fini dans create_orchestrator_graph
    
    # Par d√©faut, on utilise le chatbot g√©n√©rique
    print("[DECIDE NEXT NODE] Utilisation du chatbot g√©n√©rique par d√©faut")
    return "generic_chatbot_agent"  # Retourne le nom du n≈ìud tel que d√©fini dans create_orchestrator_graph

# 4. Construire le graphe de l'orchestrateur
def create_orchestrator_graph():
    workflow = StateGraph(OrchestratorState)

    # Ajouter les n≈ìuds
    workflow.add_node("router", route_question)
    workflow.add_node("generic_chatbot_agent", invoke_generic_chatbot_agent)
    workflow.add_node("incident_analysis_agent", invoke_incident_analysis_agent)
    workflow.add_node("fallback", fallback_node)

    # D√©finir le point d'entr√©e
    workflow.set_entry_point("router")

    # Ajouter les ar√™tes conditionnelles
    workflow.add_conditional_edges(
        "router",
        decide_next_node,
        {
            "generic_chatbot_agent": "generic_chatbot_agent",
            "incident_analysis_agent": "incident_analysis_agent",
            "fallback": "fallback",
        },
    )

    # Ajouter les ar√™tes de sortie
    workflow.add_edge("generic_chatbot_agent", END)
    workflow.add_edge("incident_analysis_agent", END)
    workflow.add_edge("fallback", END)
    
    # Compiler le workflow
    app = workflow.compile()
    
    # Fonction pour initialiser l'√©tat si n√©cessaire
    def _init_state(state):
        if not state.get("conversation_context"):
            state["conversation_context"] = {}
        if not state.get("history"):
            state["history"] = []
        if not state.get("routing_decision"):
            state["routing_decision"] = "generic_chatbot"
        if not state.get("final_response"):
            state["final_response"] = ""
        return state
    
    # Cr√©er une classe wrapper pour g√©rer l'invocation
    class OrchestratorWrapper:
        def __init__(self, app):
            self.app = app
        
        def invoke(self, input_state):
            # Initialiser l'√©tat
            state = _init_state(input_state.copy())
            print(f"[ORCHESTRATEUR] √âtat initialis√©: {state.keys()}")
            
            # Appeler la m√©thode invoke de l'application
            result = self.app.invoke(state)
            print(f"[ORCHESTRATEUR] R√©sultat apr√®s invocation: {result.keys() if isinstance(result, dict) else 'N/A'}")
            
            # Propagation explicite du champ meta si pr√©sent
            if isinstance(result, dict) and "meta" in result:
                result["meta"] = result["meta"]
            elif isinstance(result, dict):
                # Utilisation du routage r√©el pour d√©terminer le meta
                routing = result.get("routing_decision", state.get("routing_decision", "generic_chatbot"))
                if routing == "incident_analysis":
                    result["meta"] = {
                        "use_case": "incident_analysis",
                        "subgraph": "Analyse d'incident",
                        "incident_id": result.get("conversation_context", {}).get("incident_id")
                    }
                else:
                    result["meta"] = {
                        "use_case": "generic_chatbot",
                        "subgraph": "Chatbot g√©n√©rique"
                    }
            return result
    
    # Retourner le wrapper au lieu de l'application directement
    return OrchestratorWrapper(app)

